#!/usr/bin/env python3
"""
ä¸»è°ƒåº¦å™¨ - è¿è¡Œä¸‰ä¸ªç»´åº¦çš„æ™ºèƒ½ä½“è¯„ä¼°
å¯¹æŒ‡å®šappçš„æ‰€æœ‰æµ‹è¯•äººå‘˜æäº¤çš„æ–‡æ¡£è¿›è¡Œå……åˆ†æ€§ã€æ–‡æœ¬è´¨é‡å’Œç«äº‰æ€§è¯„ä¼°
"""

import os
import json
import time
import shutil
from datetime import datetime
from pathlib import Path
import traceback

# å¯¼å…¥ä¸‰ä¸ªæ™ºèƒ½ä½“
from adequacy_agent import AdequacyAgent
from textual_agent import TextualDimensionAssessmentAgent, read_reports_from_excel
from competitive_agent import CompetitiveAgent, load_tester_reports_from_excel

def ensure_directory_exists(directory_path):
    """ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
    Path(directory_path).mkdir(parents=True, exist_ok=True)

def read_requirements_document(requirements_file_path):
    """è¯»å–éœ€æ±‚æ–‡æ¡£å†…å®¹"""
    try:
        with open(requirements_file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        print(f"è­¦å‘Šï¼šéœ€æ±‚æ–‡æ¡£ {requirements_file_path} ä¸å­˜åœ¨")
        return ""
    except Exception as e:
        print(f"è¯»å–éœ€æ±‚æ–‡æ¡£æ—¶å‡ºé”™: {e}")
        return ""

def get_excel_files_count(directory_path):
    """è·å–ç›®å½•ä¸­Excelæ–‡ä»¶çš„æ•°é‡"""
    if not os.path.exists(directory_path):
        return 0
    excel_files = [f for f in os.listdir(directory_path) if f.endswith('.xlsx') and f[0].isdigit()]
    return len(excel_files)

def read_all_test_cases_from_directory(directory_path):
    """ä»ç›®å½•ä¸­è¯»å–æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶"""
    all_test_cases = []
    if not os.path.exists(directory_path):
        print(f"è­¦å‘Šï¼šæµ‹è¯•ç”¨ä¾‹ç›®å½• {directory_path} ä¸å­˜åœ¨")
        return all_test_cases
    
    excel_files = [f for f in os.listdir(directory_path) if f.endswith('.xlsx') and f[0].isdigit()]
    excel_files.sort(key=lambda x: int(x.split('.')[0]))  # æŒ‰æ•°å­—æ’åº
    
    for file_name in excel_files:
        file_path = os.path.join(directory_path, file_name)
        try:
            print(f"  è¯»å–æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶: {file_name}")
            test_cases = read_reports_from_excel(file_path, "test_case")
            # ä¸ºæ¯ä¸ªæµ‹è¯•ç”¨ä¾‹æ·»åŠ æ–‡ä»¶æ¥æºä¿¡æ¯
            for tc in test_cases:
                tc['source_file'] = file_name
                tc['tester_id'] = file_name.split('.')[0]
            all_test_cases.extend(test_cases)
        except Exception as e:
            print(f"  è¯»å–æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {e}")
    
    return all_test_cases

def read_all_defects_from_directory(directory_path):
    """ä»ç›®å½•ä¸­è¯»å–æ‰€æœ‰ç¼ºé™·æ–‡ä»¶"""
    all_defects = []
    if not os.path.exists(directory_path):
        print(f"è­¦å‘Šï¼šç¼ºé™·ç›®å½• {directory_path} ä¸å­˜åœ¨")
        return all_defects
    
    excel_files = [f for f in os.listdir(directory_path) if f.endswith('.xlsx') and f[0].isdigit()]
    excel_files.sort(key=lambda x: int(x.split('.')[0]))  # æŒ‰æ•°å­—æ’åº
    
    for file_name in excel_files:
        file_path = os.path.join(directory_path, file_name)
        try:
            print(f"  è¯»å–ç¼ºé™·æ–‡ä»¶: {file_name}")
            defects = read_reports_from_excel(file_path, "defect")
            # ä¸ºæ¯ä¸ªç¼ºé™·æ·»åŠ æ–‡ä»¶æ¥æºä¿¡æ¯
            for defect in defects:
                defect['source_file'] = file_name
                defect['tester_id'] = file_name.split('.')[0]
            all_defects.extend(defects)
        except Exception as e:
            print(f"  è¯»å–ç¼ºé™·æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {e}")
    
    return all_defects

def group_test_cases_by_tester(test_cases):
    """æŒ‰æµ‹è¯•äººå‘˜åˆ†ç»„æµ‹è¯•ç”¨ä¾‹"""
    tester_groups = {}
    for tc in test_cases:
        tester_id = tc.get('tester_id', 'unknown')
        if tester_id not in tester_groups:
            tester_groups[tester_id] = []
        tester_groups[tester_id].append(tc)
    return tester_groups

def run_adequacy_assessment(requirements_text, test_cases, temp_dir, results_dir):
    """è¿è¡Œå……åˆ†æ€§è¯„ä¼° - åˆ†åˆ«è¯„ä¼°æ¯ä¸ªæµ‹è¯•äººå‘˜çš„è¦†ç›–ç‡"""
    print("\n" + "="*60)
    print("1. å¼€å§‹å……åˆ†æ€§è¯„ä¼° (Adequacy Assessment)")
    print("="*60)
    
    try:
        # åˆå§‹åŒ–å……åˆ†æ€§è¯„ä¼°æ™ºèƒ½ä½“
        adequacy_agent = AdequacyAgent()
        
        print(f"éœ€æ±‚æ–‡æ¡£é•¿åº¦: {len(requirements_text)} å­—ç¬¦")
        print(f"æµ‹è¯•ç”¨ä¾‹æ€»æ•°: {len(test_cases)}")
        
        # æ­¥éª¤1ï¼šæ„å»ºéœ€æ±‚æ ‘ï¼ˆå…±åŒçš„ï¼‰
        print("\nğŸŒ³ æ­¥éª¤1ï¼šæ„å»ºéœ€æ±‚æ ‘...")
        requirement_tree = adequacy_agent.analyze_requirements_to_tree(requirements_text)
        if not requirement_tree:
            print("âŒ éœ€æ±‚æ ‘æ„å»ºå¤±è´¥")
            return {"error": "Failed to build requirement tree."}
        
        # ç«‹å³ä¿å­˜éœ€æ±‚æ ‘åˆ°tempç›®å½•
        tree_file = os.path.join(temp_dir, "requirement_tree.json")
        with open(tree_file, 'w', encoding='utf-8') as f:
            json.dump(adequacy_agent.requirement_tree, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… éœ€æ±‚æ ‘æ„å»ºå®Œæˆ!")
        print(f"ğŸ“Š éœ€æ±‚æ ‘ç»Ÿè®¡: {len(requirement_tree)} ä¸ªèŠ‚ç‚¹, {len(adequacy_agent.get_leaf_nodes())} ä¸ªå¶å­èŠ‚ç‚¹")
        print(f"ğŸ’¾ éœ€æ±‚æ ‘å·²ä¿å­˜åˆ°: {tree_file}")
        
        # æ­¥éª¤2ï¼šæŒ‰æµ‹è¯•äººå‘˜åˆ†ç»„æµ‹è¯•ç”¨ä¾‹
        print("\nğŸ‘¥ æ­¥éª¤2ï¼šæŒ‰æµ‹è¯•äººå‘˜åˆ†ç»„æµ‹è¯•ç”¨ä¾‹...")
        tester_groups = group_test_cases_by_tester(test_cases)
        print(f"æ‰¾åˆ° {len(tester_groups)} ä½æµ‹è¯•äººå‘˜")
        
        # æ­¥éª¤3ï¼šåˆ†åˆ«è¯„ä¼°æ¯ä¸ªæµ‹è¯•äººå‘˜çš„è¦†ç›–ç‡
        print("\nğŸ“Š æ­¥éª¤3ï¼šåˆ†åˆ«è¯„ä¼°æ¯ä¸ªæµ‹è¯•äººå‘˜çš„è¦†ç›–ç‡...")
        tester_coverage_reports = {}
        
        for tester_id, tester_test_cases in tester_groups.items():
            print(f"\nğŸ” è¯„ä¼°æµ‹è¯•äººå‘˜ {tester_id} (å…±{len(tester_test_cases)}ä¸ªæµ‹è¯•ç”¨ä¾‹)...")
            
            # æ˜ å°„è¯¥æµ‹è¯•äººå‘˜çš„æµ‹è¯•ç”¨ä¾‹
            mapped_test_cases = adequacy_agent.map_test_cases_to_tree(tester_test_cases, requirement_tree)
            if not mapped_test_cases and tester_test_cases:
                print(f"  è­¦å‘Šï¼šæµ‹è¯•äººå‘˜ {tester_id} çš„æµ‹è¯•ç”¨ä¾‹æ²¡æœ‰æˆåŠŸæ˜ å°„")
            
            # è¯„ä¼°è¯¥æµ‹è¯•äººå‘˜çš„è¦†ç›–ç‡
            coverage_report = adequacy_agent.evaluate_coverage(requirement_tree, mapped_test_cases)
            coverage_report["tester_id"] = tester_id
            coverage_report["test_cases_count"] = len(tester_test_cases)
            
            tester_coverage_reports[tester_id] = coverage_report
            
            print(f"  æµ‹è¯•äººå‘˜ {tester_id} è¦†ç›–ç‡: {coverage_report.get('coverage_percentage', 'N/A')}")
        
        # ä¿å­˜æ¯ä¸ªæµ‹è¯•äººå‘˜çš„è¦†ç›–ç‡æŠ¥å‘Š
        adequacy_result_file = os.path.join(results_dir, "adequacy_assessment_result.json")
        with open(adequacy_result_file, 'w', encoding='utf-8') as f:
            json.dump(tester_coverage_reports, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆæ€»ä½“ç»Ÿè®¡æ‘˜è¦
        summary = {
            "total_testers": len(tester_groups),
            "requirement_tree_stats": {
                "total_nodes": len(requirement_tree),
                "leaf_nodes": len(adequacy_agent.get_leaf_nodes())
            },
            "coverage_stats": {
                "average_coverage": 0,
                "max_coverage": 0,
                "min_coverage": 100,
                "coverage_distribution": {}
            }
        }
        
        if tester_coverage_reports:
            coverages = []
            for tester_id, report in tester_coverage_reports.items():
                coverage_str = report.get('coverage_percentage', '0%')
                coverage_num = float(coverage_str.replace('%', ''))
                coverages.append(coverage_num)
                summary["coverage_stats"]["coverage_distribution"][tester_id] = coverage_num
            
            summary["coverage_stats"]["average_coverage"] = sum(coverages) / len(coverages)
            summary["coverage_stats"]["max_coverage"] = max(coverages)
            summary["coverage_stats"]["min_coverage"] = min(coverages)
        
        # ä¿å­˜æ€»ä½“ç»Ÿè®¡æ‘˜è¦
        summary_file = os.path.join(results_dir, "adequacy_assessment_summary.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… å……åˆ†æ€§è¯„ä¼°å®Œæˆ!")
        print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"  - æµ‹è¯•äººå‘˜æ•°é‡: {summary['total_testers']}")
        print(f"  - å¹³å‡è¦†ç›–ç‡: {summary['coverage_stats']['average_coverage']:.2f}%")
        print(f"  - æœ€é«˜è¦†ç›–ç‡: {summary['coverage_stats']['max_coverage']:.2f}%")
        print(f"  - æœ€ä½è¦†ç›–ç‡: {summary['coverage_stats']['min_coverage']:.2f}%")
        print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {adequacy_result_file}")
        print(f"ğŸ“Š ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
        
        return tester_coverage_reports
        
    except Exception as e:
        print(f"âŒ å……åˆ†æ€§è¯„ä¼°å¤±è´¥: {e}")
        traceback.print_exc()
        return {"error": str(e)}

def run_textual_assessment(test_cases, defects, temp_dir, results_dir):
    """è¿è¡Œæ–‡æœ¬è´¨é‡è¯„ä¼° - æŒ‰æµ‹è¯•äººå‘˜åˆ†åˆ«è¯„ä¼°"""
    print("\n" + "="*60)
    print("2. å¼€å§‹æ–‡æœ¬è´¨é‡è¯„ä¼° (Textual Assessment)")
    print("="*60)
    
    try:
        # åˆå§‹åŒ–æ–‡æœ¬è´¨é‡è¯„ä¼°æ™ºèƒ½ä½“
        textual_agent = TextualDimensionAssessmentAgent()
        
        print(f"æµ‹è¯•ç”¨ä¾‹æ•°: {len(test_cases)}")
        print(f"ç¼ºé™·æŠ¥å‘Šæ•°: {len(defects)}")
        
        # æŒ‰æµ‹è¯•äººå‘˜åˆ†ç»„æ•°æ®
        print("\nğŸ‘¥ æŒ‰æµ‹è¯•äººå‘˜åˆ†ç»„æ•°æ®...")
        tester_testcases = {}
        tester_defects = {}
        
        # åˆ†ç»„æµ‹è¯•ç”¨ä¾‹
        for tc in test_cases:
            tester_id = tc.get('tester_id', 'unknown')
            if tester_id not in tester_testcases:
                tester_testcases[tester_id] = []
            tester_testcases[tester_id].append(tc)
        
        # åˆ†ç»„ç¼ºé™·
        for defect in defects:
            tester_id = defect.get('tester_id', 'unknown')
            if tester_id not in tester_defects:
                tester_defects[tester_id] = []
            tester_defects[tester_id].append(defect)
        
        # è·å–æ‰€æœ‰æµ‹è¯•äººå‘˜
        all_tester_ids = set(tester_testcases.keys()) | set(tester_defects.keys())
        print(f"æ‰¾åˆ° {len(all_tester_ids)} ä½æµ‹è¯•äººå‘˜")
        
        # åˆ†åˆ«è¯„ä¼°æ¯ä¸ªæµ‹è¯•äººå‘˜
        tester_evaluations = {}
        overall_stats = {
            "total_testers": len(all_tester_ids),
            "successful_evaluations": 0,
            "failed_evaluations": 0,
            "testcase_scores": [],
            "defect_scores": [],
            "overall_scores": []
        }
        
        for tester_id in sorted(all_tester_ids):
            tester_testcases_list = tester_testcases.get(tester_id, [])
            tester_defects_list = tester_defects.get(tester_id, [])
            
            print(f"\nğŸ” è¯„ä¼°æµ‹è¯•äººå‘˜ {tester_id}:")
            print(f"  - æµ‹è¯•ç”¨ä¾‹: {len(tester_testcases_list)} ä¸ª")
            print(f"  - ç¼ºé™·æŠ¥å‘Š: {len(tester_defects_list)} ä¸ª")
            
            # ä½¿ç”¨æ–°çš„æŠ½æ ·è¯„ä¼°æ–¹æ³•
            tester_result = textual_agent.evaluate_tester_reports_with_sampling(
                tester_id, 
                tester_testcases_list, 
                tester_defects_list
            )
            
            tester_evaluations[tester_id] = tester_result
            
            # æ›´æ–°æ€»ä½“ç»Ÿè®¡
            if tester_result.get("success"):
                overall_stats["successful_evaluations"] += 1
                score_stats = tester_result.get("score_statistics", {})
                
                if score_stats.get("testcase_avg_score", 0) > 0:
                    overall_stats["testcase_scores"].append(score_stats["testcase_avg_score"])
                if score_stats.get("defect_avg_score", 0) > 0:
                    overall_stats["defect_scores"].append(score_stats["defect_avg_score"])
                if score_stats.get("overall_avg_score", 0) > 0:
                    overall_stats["overall_scores"].append(score_stats["overall_avg_score"])
            else:
                overall_stats["failed_evaluations"] += 1
        
        # è®¡ç®—æ€»ä½“å¹³å‡åˆ†
        overall_stats["avg_testcase_score"] = sum(overall_stats["testcase_scores"]) / len(overall_stats["testcase_scores"]) if overall_stats["testcase_scores"] else 0
        overall_stats["avg_defect_score"] = sum(overall_stats["defect_scores"]) / len(overall_stats["defect_scores"]) if overall_stats["defect_scores"] else 0
        overall_stats["avg_overall_score"] = sum(overall_stats["overall_scores"]) / len(overall_stats["overall_scores"]) if overall_stats["overall_scores"] else 0
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        final_result_file = os.path.join(results_dir, "textual_assessment_result.json")
        with open(final_result_file, 'w', encoding='utf-8') as f:
            json.dump(tester_evaluations, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ€»ä½“ç»Ÿè®¡æ‘˜è¦
        summary_file = os.path.join(results_dir, "textual_assessment_summary.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(overall_stats, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… æ–‡æœ¬è´¨é‡è¯„ä¼°å®Œæˆ!")
        print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"  - æµ‹è¯•äººå‘˜æ•°é‡: {overall_stats['total_testers']}")
        print(f"  - æˆåŠŸè¯„ä¼°: {overall_stats['successful_evaluations']}")
        print(f"  - å¤±è´¥è¯„ä¼°: {overall_stats['failed_evaluations']}")
        print(f"  - æµ‹è¯•ç”¨ä¾‹å¹³å‡åˆ†: {overall_stats['avg_testcase_score']:.2f}%")
        print(f"  - ç¼ºé™·å¹³å‡åˆ†: {overall_stats['avg_defect_score']:.2f}%")
        print(f"  - æ€»ä½“å¹³å‡åˆ†: {overall_stats['avg_overall_score']:.2f}%")
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {final_result_file}")
        
        return tester_evaluations
        
    except Exception as e:
        print(f"âŒ æ–‡æœ¬è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
        traceback.print_exc()
        return {"error": str(e)}

def run_competitive_assessment(defects_dir, temp_dir, results_dir):
    """è¿è¡Œç«äº‰æ€§è¯„ä¼°"""
    print("\n" + "="*60)
    print("3. å¼€å§‹ç«äº‰æ€§è¯„ä¼° (Competitive Assessment)")
    print("="*60)
    
    try:
        # åˆå§‹åŒ–ç«äº‰æ€§è¯„ä¼°æ™ºèƒ½ä½“
        competitive_agent = CompetitiveAgent()
        
        # è·å–ç¼ºé™·æ–‡ä»¶æ•°é‡
        excel_files_count = get_excel_files_count(defects_dir)
        print(f"ç¼ºé™·æ–‡ä»¶æ•°é‡: {excel_files_count}")
        
        if excel_files_count == 0:
            print("è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°ç¼ºé™·æ–‡ä»¶ï¼Œè·³è¿‡ç«äº‰æ€§è¯„ä¼°")
            return {"error": "æ²¡æœ‰æ‰¾åˆ°ç¼ºé™·æ–‡ä»¶"}
        
        # åŠ è½½æµ‹è¯•äººå‘˜æŠ¥å‘Šæ•°æ®
        tester_reports_data = load_tester_reports_from_excel(defects_dir, excel_files_count)
        print(f"åŠ è½½äº† {len(tester_reports_data)} ä½æµ‹è¯•äººå‘˜çš„ç¼ºé™·æ•°æ®")
        
        # æ‰§è¡Œç«äº‰æ€§è¯„ä¼°
        evaluation_results, cluster_tree_root = competitive_agent.evaluate_reports(tester_reports_data)
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        competitive_result_file = os.path.join(results_dir, "competitive_assessment_result.json")
        with open(competitive_result_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜èšç±»æ ‘åˆ°tempç›®å½•
        if cluster_tree_root:
            tree_file = os.path.join(temp_dir, "defect_cluster_tree.json")
            with open(tree_file, 'w', encoding='utf-8') as f:
                json.dump(cluster_tree_root.to_dict(), f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç«äº‰æ€§è¯„ä¼°å®Œæˆ!")
        print(f"è¯„ä¼°äº† {len(evaluation_results)} ä½æµ‹è¯•äººå‘˜")
        if evaluation_results:
            avg_score = sum(r.get('competitive_score', 0) for r in evaluation_results) / len(evaluation_results)
            print(f"å¹³å‡ç«äº‰æ€§å¾—åˆ†: {avg_score:.2f}")
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {competitive_result_file}")
        
        return evaluation_results
        
    except Exception as e:
        print(f"âŒ ç«äº‰æ€§è¯„ä¼°å¤±è´¥: {e}")
        traceback.print_exc()
        return {"error": str(e)}

def generate_summary_report(adequacy_result, textual_result, competitive_result, results_dir):
    """ç”Ÿæˆç»¼åˆæ‘˜è¦æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ç”Ÿæˆç»¼åˆæ‘˜è¦æŠ¥å‘Š")
    print("="*60)
    
    # å¤„ç†æ–°çš„å……åˆ†æ€§è¯„ä¼°ç»“æœç»“æ„ï¼ˆæŒ‰æµ‹è¯•äººå‘˜åˆ†ç»„ï¼‰
    adequacy_summary = {
        "status": "success" if "error" not in adequacy_result else "failed",
        "error": adequacy_result.get("error")
    }
    
    if "error" not in adequacy_result and adequacy_result:
        # ä»å……åˆ†æ€§è¯„ä¼°æ‘˜è¦æ–‡ä»¶ä¸­è¯»å–ç»Ÿè®¡ä¿¡æ¯
        adequacy_summary_file = os.path.join(results_dir, "adequacy_assessment_summary.json")
        if os.path.exists(adequacy_summary_file):
            try:
                with open(adequacy_summary_file, 'r', encoding='utf-8') as f:
                    adequacy_stats = json.load(f)
                adequacy_summary.update(adequacy_stats)
            except Exception as e:
                print(f"è¯»å–å……åˆ†æ€§è¯„ä¼°æ‘˜è¦æ—¶å‡ºé”™: {e}")
        
        # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼ˆæŒ‰æµ‹è¯•äººå‘˜åˆ†ç»„ï¼‰ï¼Œè®¡ç®—ä¸€äº›åŸºæœ¬ç»Ÿè®¡
        if isinstance(adequacy_result, dict) and not adequacy_result.get("error"):
            adequacy_summary["tester_count"] = len(adequacy_result)
            
            # è®¡ç®—è¦†ç›–ç‡ç»Ÿè®¡
            coverages = []
            for tester_id, tester_report in adequacy_result.items():
                if isinstance(tester_report, dict) and "coverage_percentage" in tester_report:
                    coverage_str = tester_report.get("coverage_percentage", "0%")
                    coverage_num = float(coverage_str.replace("%", ""))
                    coverages.append(coverage_num)
            
            if coverages:
                adequacy_summary["coverage_stats"] = {
                    "average_coverage": sum(coverages) / len(coverages),
                    "max_coverage": max(coverages),
                    "min_coverage": min(coverages)
                }
    
    summary_report = {
        "evaluation_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "adequacy_assessment": adequacy_summary,
        "textual_assessment": {
            "status": "success" if "error" not in textual_result else "failed",
            "error": textual_result.get("error")
        },
        "competitive_assessment": {
            "status": "success" if "error" not in competitive_result else "failed",
            "error": competitive_result.get("error")
        }
    }
    
    # æ·»åŠ æ–‡æœ¬è´¨é‡è¯„ä¼°çš„è¯¦ç»†ä¿¡æ¯
    if "error" not in textual_result and textual_result:
        try:
            # è¯»å–æ–‡æœ¬è´¨é‡è¯„ä¼°æ‘˜è¦
            summary_file = os.path.join(results_dir, "textual_assessment_summary.json")
            if os.path.exists(summary_file):
                with open(summary_file, 'r', encoding='utf-8') as f:
                    textual_summary = json.load(f)
                summary_report["textual_assessment"].update(textual_summary)
            
            # å¦‚æœæ˜¯æŒ‰æµ‹è¯•äººå‘˜åˆ†ç»„çš„ç»“æœï¼Œæ·»åŠ æµ‹è¯•äººå‘˜ç»Ÿè®¡ä¿¡æ¯
            if isinstance(textual_result, dict):
                summary_report["textual_assessment"]["tester_count"] = len(textual_result)
                
                # è®¡ç®—æµ‹è¯•äººå‘˜å¹³å‡åˆ†ç»Ÿè®¡
                tester_scores = []
                for tester_id, tester_result in textual_result.items():
                    if tester_result.get("success") and "score_statistics" in tester_result:
                        overall_score = tester_result["score_statistics"].get("overall_avg_score", 0)
                        if overall_score > 0:
                            tester_scores.append(overall_score)
                
                if tester_scores:
                    summary_report["textual_assessment"]["tester_score_stats"] = {
                        "average_score": sum(tester_scores) / len(tester_scores),
                        "max_score": max(tester_scores),
                        "min_score": min(tester_scores)
                    }
                    
        except Exception as e:
            print(f"è¯»å–æ–‡æœ¬è´¨é‡è¯„ä¼°æ‘˜è¦æ—¶å‡ºé”™: {e}")
    
    # æ·»åŠ ç«äº‰æ€§è¯„ä¼°çš„è¯¦ç»†ä¿¡æ¯
    if "error" not in competitive_result and competitive_result:
        if isinstance(competitive_result, list):
            summary_report["competitive_assessment"]["total_testers"] = len(competitive_result)
            if competitive_result:
                avg_score = sum(r.get('competitive_score', 0) for r in competitive_result) / len(competitive_result)
                summary_report["competitive_assessment"]["average_competitive_score"] = round(avg_score, 2)
                max_score = max(r.get('competitive_score', 0) for r in competitive_result)
                summary_report["competitive_assessment"]["max_competitive_score"] = max_score
    
    # ä¿å­˜ç»¼åˆæ‘˜è¦
    summary_file = os.path.join(results_dir, "comprehensive_summary.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_report, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“Š ç»¼åˆæ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜åˆ°: {summary_file}")
    return summary_report

def main(app_name="app1"):
    """ä¸»å‡½æ•° - è°ƒåº¦æ‰€æœ‰æ™ºèƒ½ä½“è¿›è¡Œè¯„ä¼°"""
    print("ğŸš€ å¼€å§‹è¿è¡Œä¸‰ç»´åº¦æ™ºèƒ½ä½“è¯„ä¼°ç³»ç»Ÿ")
    print(f"ç›®æ ‡åº”ç”¨: {app_name}")
    print("="*80)
    
    start_time = time.time()
    
    # è®¾ç½®è·¯å¾„
    app_base_dir = f"data/{app_name}"
    testcases_dir = os.path.join(app_base_dir, "testcases")
    defects_dir = os.path.join(app_base_dir, "defects")
    requirements_file = os.path.join("data", "test_requirements.txt")  # éœ€æ±‚æ–‡æ¡£åœ¨dataæ ¹ç›®å½•
    
    results_dir = os.path.join(app_base_dir, "results")
    temp_dir = os.path.join(app_base_dir, "temp")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    ensure_directory_exists(results_dir)
    ensure_directory_exists(temp_dir)
    
    print(f"ğŸ“ ç»“æœç›®å½•: {results_dir}")
    print(f"ğŸ“ ä¸´æ—¶ç›®å½•: {temp_dir}")
    
    # æ£€æŸ¥å¿…è¦çš„ç›®å½•å’Œæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(app_base_dir):
        print(f"âŒ é”™è¯¯: åº”ç”¨ç›®å½• {app_base_dir} ä¸å­˜åœ¨")
        return
    
    # è¯»å–éœ€æ±‚æ–‡æ¡£
    print(f"\nğŸ“– è¯»å–éœ€æ±‚æ–‡æ¡£: {requirements_file}")
    requirements_text = read_requirements_document(requirements_file)
    if not requirements_text:
        print("âš ï¸  è­¦å‘Š: éœ€æ±‚æ–‡æ¡£ä¸ºç©ºæˆ–è¯»å–å¤±è´¥ï¼Œå……åˆ†æ€§è¯„ä¼°å¯èƒ½å—å½±å“")
    
    # è¯»å–æµ‹è¯•ç”¨ä¾‹
    print(f"\nğŸ“‹ è¯»å–æµ‹è¯•ç”¨ä¾‹ç›®å½•: {testcases_dir}")
    test_cases = read_all_test_cases_from_directory(testcases_dir)
    print(f"å…±åŠ è½½ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    
    # è¯»å–ç¼ºé™·æŠ¥å‘Š
    print(f"\nğŸ› è¯»å–ç¼ºé™·ç›®å½•: {defects_dir}")
    defects = read_all_defects_from_directory(defects_dir)
    print(f"å…±åŠ è½½ {len(defects)} ä¸ªç¼ºé™·æŠ¥å‘Š")
    
    # æ‰§è¡Œä¸‰ä¸ªç»´åº¦çš„è¯„ä¼°
    adequacy_result = {"error": "è·³è¿‡å……åˆ†æ€§è¯„ä¼°"}  # å ä½ç¬¦
    # adequacy_result = run_adequacy_assessment(requirements_text, test_cases, temp_dir, results_dir)
    competitive_result = run_competitive_assessment(defects_dir, temp_dir, results_dir)
    textual_result = {"error": "è·³è¿‡æ–‡æœ¬è´¨é‡è¯„ä¼°"}  # å ä½ç¬¦
    # textual_result = run_textual_assessment(test_cases, defects, temp_dir, results_dir)
    
    # ç”Ÿæˆç»¼åˆæ‘˜è¦æŠ¥å‘Š
    # summary_report = generate_summary_report(adequacy_result, textual_result, competitive_result, results_dir)
    
    # è®¡ç®—æ€»è€—æ—¶
    end_time = time.time()
    total_time = round(end_time - start_time, 2)
    
    print("\n" + "="*80)
    print("ğŸ‰ ä¸‰ç»´åº¦è¯„ä¼°å®Œæˆ!")
    print("="*80)
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time} ç§’")
    print(f"ğŸ“Š å……åˆ†æ€§è¯„ä¼°: {'â­ï¸ å·²è·³è¿‡' if adequacy_result.get('error') == 'è·³è¿‡å……åˆ†æ€§è¯„ä¼°' else ('âœ…' if adequacy_result.get('error') is None else 'âŒ')}")
    print(f"ğŸ“ æ–‡æœ¬è´¨é‡è¯„ä¼°: {'â­ï¸ å·²è·³è¿‡' if textual_result.get('error') == 'è·³è¿‡æ–‡æœ¬è´¨é‡è¯„ä¼°' else ('âœ…' if not isinstance(textual_result, dict) or textual_result.get('error') is None else 'âŒ')}")
    print(f"ğŸ† ç«äº‰æ€§è¯„ä¼°: {'âœ…' if not isinstance(competitive_result, dict) or competitive_result.get('error') is None else 'âŒ'}")
    print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {results_dir}")
    print("="*80)

if __name__ == "__main__":
    import sys
    
    # æ”¯æŒå‘½ä»¤è¡Œå‚æ•°æŒ‡å®šappåç§°
    app_name = sys.argv[1] if len(sys.argv) > 1 else "app1"
    
    try:
        main(app_name)
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­äº†è¯„ä¼°è¿‡ç¨‹")
    except Exception as e:
        print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
        traceback.print_exc() 